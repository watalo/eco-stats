# 第5章 多元线性回归

## 5.1 二元线性回归

### 案例说明

Cobb-Dougls生成函数：$$y_i=\alpha k_i^{\beta}l_i^{\gamma}e^{\epsilon_i}$$

两边同时取对数，可转换为线性模型：$$\ln y_i=\ln \alpha +\beta\ln k_i +\gamma \ln l_i + \epsilon_i$$
这就是二元线性回归模型。
### 代码实现

> [[Chapter_05.ipynb]]

## 5.2 多元线性回归模型

多元线性回归模型：$$y_i=\beta_1x_{i1}+\beta_2x_{i2}+\dots+\beta_Kx_{iK}+\epsilon_i \quad (i=1，\dots，n)$$
使用矩阵表示：$$y \equiv X\beta+\epsilon$$
其中：$$\mathbf X \equiv \begin{pmatrix}
1& x_{12}& \cdots & x_{1K}\\
1& x_{22}& \cdots & x_{2K}\\
\vdots&\vdots&\ddots&\vdots\\
1& x_{n2}& \cdots & x_{nK}\\
\end{pmatrix}_{n \times K}$$
$x_{i1}=1$，即可转化为常数项。
## 5.3 OLS估计量的推导

> 使用矩阵表示多元线性回归模型简洁明了。

目标函数：$$\min_{\hat \beta_1,\dots,\hat \beta_k} \sum_{i=1}^n e_i^2= \sum_{i=1}^n(y_i-\hat\beta_1-\hat\beta_2x_{i2}-\hat\beta_3x_{i3}-\dots-\hat\beta_Kx_{iK})^2$$
- 找到$(\hat \beta_1,\hat \beta_2,\dots,\hat \beta_K)$使残差平方和（SSR）最小。

分别求偏导，得到正规方程组：$$\mathbf X' \mathbf e=0$$
- 残差向量 $\mathbf e \equiv (e_1 \quad e_2 \dots e_n)'$ 与每个解释变量均正交。
- 将 $\mathbf e$ 表示为：$$\mathbf e = \mathbf y -\mathbf X \hat \beta$$
- 带入正规方程组，求解**OLS估计量**为：$$\hat \beta \equiv (\mathbf X’ \mathbf X)^{-1}\mathbf X'y$$
## 5.4 OLS的几何解释

### 正交性
被解释变量 $y_i$ 的拟合值（fitted value）/预测值（predicted value）为 $\hat y_i$ ，有：$$\hat y_i \equiv \hat\beta_1+\hat\beta_2x_{i2}+\hat\beta_3x_{i3}+\dots+\hat\beta_Kx_{iK} \quad (i=1,\dots,n)$$ 
用列向量表示所有个体的拟合值为$\hat y$：$$\mathbf {\hat y} \equiv \mathbf X \mathbf {\hat \beta} $$
拟合值向量与残差向量正交：$$\mathbf {\hat y}'e=(\mathbf X \mathbf {\hat \beta})'e=\mathbf{\hat \beta}'\mathbf X 'e=0$$
### 线性投影
因为$\mathbf e=\mathbf y- \mathbf X \mathbf{\hat \beta}=\mathbf y - \mathbf {\hat y}$，故：$$\mathbf y = \mathbf {\hat y}+\mathbf e$$
- 拟合值 $\hat y$ 是被解释变量 $y$ 向解释变量超平面 $X$ 的线性投影（Linear projection）
- 残差 $e$ 则是从投影处，垂直于X超平面指向 $y$ 的直线
![[5-4OLS的几何解释_投影.png]]

## 5.5 拟合优度

![[4_一元线性回归#^4adc1c]]
- 拟合优度在 $[0，1]$ 之间

通过增加解释变量数和优化新增解释变量（以及已有解释变量）的系数，都可以提高$R^2$。因此，引入校正拟合优度来对解释变量过多（模型不够简洁）进行惩罚。
### 定义
校正拟合优度 $\overline R^2$  (Adjusted $R^2$) 为：$$\overline R^2 \equiv 1 - \frac{\frac{1}{n-K}\sum_{i=1}^ne_i^2}{\frac{1}{n-1}\sum_{i=1}^n(y_i-\overline y)^2}$$
- $\sum_{i=1}^ne_i^2$的自由度（degree of freedom）n-K：n个变量受K个正规方程约束
- $\sum_{i=1}^n(y_i-\overline y)^2$的自由度（n-1）

> 缺点：可能是负值
## 5.6 古典线性回归模型的假定

#假定 5.1  线性假定（Linearity）
- 线性假定的本质是：回归函数是参数的线性函数
- 如果变量的边际效用不是常数，可考虑加入平方项
#假定 5.2  严格外生性（Strict exogeneity）
- 解释变量 和 被解释变量 相互独立
#假定 5.2  不存在严格多重共线性（strict multicolinearity）
- 解释变量之间是独立的

```ad-note
title:矩阵的秩（Rank）
矩阵的秩（Rank）是指矩阵中线性无关的行或列的最大数目。当一个矩阵是满秩的，意味着它的秩等于它的行数或列数中的较小者。

具体来说，对于一个 $( m \times n)$ 的矩阵 $( A )$，如果矩阵$( A )$ 的所有$ ( m ) $行（或者所有 $( n )$ 列）都是线性无关的，那么我们就说这个矩阵是满秩的。对于方阵（即行数和列数相等的矩阵），满秩意味着该矩阵是可逆的，也就是说存在一个逆矩阵$A^{-1}$ 使得 $(AA^{-1} = A^{-1}A = I)$ ，其中 \( I \) 是单位矩阵。

在统计学中，如果一个数据矩阵是满秩的，那么可以通过最小二乘法来估计回归模型的参数。

总结一下，矩阵满秩意味着矩阵中的行向量或列向量都是线性无关的，这通常与系统的可解性、系统的控制性和数据的估计能力等重要性质相关联。
```
## 5.7 OLS的小样本性质

OLS估计量 $\hat \beta$ 是样本数据的函数，也是随机变量，其分布函数为抽样分布(sampling ditribution)。
古典线性回归模型假定下，OLS估计量有如下性质：
### （1）线性性
OLS估计量 $\hat \beta$ 可视为 $y$ 的线性组合，将 $(\mathbf X’ \mathbf X)^{-1}\mathbf X'$ 视为系数矩阵，故是线性估计量。
### （2）无偏性
$\hat \beta$ 不会系统地高估或低估 $\beta$，$E(\hat \beta|X)=\beta$

证明：$$\begin{equation}\begin{split} 
\hat \beta - \beta&= (X'X)^{-1}X'y-\beta\\ 
&=(X'X)^{-1}X'(X\beta+\epsilon)-\beta\\
&=\beta-\beta+(X'X)^{-1}X'\epsilon\\
&=(X'X)^{-1}X'\epsilon
\end{split}\end{equation}$$
定义 $A \equiv (X'X)^{-1}X'$，上式两边对X求条件期望得 $E(\hat \beta|X)=\beta$
进一步还可以得到：$E(\hat \beta)=E_XE(\hat \beta|X)=E_X(\beta)=\beta$
### （3）估计量 $\hat \beta$ 的协方差矩阵
#假定  5.4 球形扰动项$Var(\epsilon | X)= \sigma^2 \mathbf I_n$，即扰动项满足：
- 同方差
- 无自相关性
其中：$$Var(\epsilon | X)= \sigma^2 \mathbf I_n=\begin{pmatrix}\sigma^2 \quad 0 \quad \dots \quad0 \\
0 \quad \sigma^2 \quad \dots \quad 0 \\ \vdots \quad \vdots \quad \ddots \quad \vdots \\ 0 \quad 0 \quad \dots \quad \sigma^2\end{pmatrix}$$

> ##### 定义
> **条件同方差(Conditional homoskedasticity)**：主对角线元素均相同
> **条件异方差(Conditional Heterskedasticity)**：主对角线元素不完全相同
> **自相关(autocorrelation / series correlation)**：非对角线元素不全为0

则有：$$Var(\hat  \beta|X)=\sigma^2(X'X)^{-1}$$
引入球形扰动项的好处：
- 证明上式的必要条件
- OLS在某种范围内是最有效的估计量
### （4）高斯-马尔可夫定理(Gauss-Markov Theorem)
在假定5.1-5.4均成立时，最小二乘法是最佳线性物品估计(Best Linear Unbiased Estimator, BLUE)。
- 在所有的线性的无偏估计中，最小二乘法的方差最小。
### （5）对扰动项方差的无偏估计
扰动项方差 $\sigma^2 = Var(\epsilon_i)$  可由**回归方程的标准误**的二次方来无偏估计。

扰动项$\epsilon_i$ 不可观测，将残差 $e_i$ 视为其实现值，可以得到$\sigma^2$的无偏估计：$$s^2 \equiv \frac{1}{n-K}\sum_{i=1}^n e_i^2$$
> ##### 回归方程的标准误
> $s=\sqrt {s^2}$ 为**回归方程的标准误差(standard error of the regression)**，简称**回归方程的标准误**。用来衡量回归方程扰动项的波动幅度。

因此，OLS估计量 $\hat \beta$ 的协方差矩阵可以用 $s^2(X'X)^{-1}$来估计。

> ##### 估计量的标准误
 $\sqrt{s^2(X'X)_{kk}^{-1}}$ 为OLS估计量 $\hat \beta_k$ 的标准误差，简称标准误，记为$SE(\hat \beta_k)$，即$$SE(\hat \beta_k) \equiv \sqrt{s^2(X'X)_{kk}^{-1}}$$

更一般地，称对某统计量的标准差的估计值(estimated standard deviation)为该统计值的**标准误**，作为对统计量估计误差的度量。
- 通常，在得到参数的点估计之后，还须给出相应的标准误，才能知道此点估计的准确程度。
## 5.8 对单个系数的t检验

### （1）计量经济学中的统计推断
#### 分类
计量经济学的统计推断方法分为两大类:
- 小样本理论(有限样本理论)
	- 无论样本容量是多少，小样本理论都成立，不要求样本容量 $n \to\infty$
	- 缺点是不同意推导其统计量的分布，需要对随机变量做很强的具体假定
- 大样本理论
	- 要求样本容量 $n \to\infty$
#### 检验方法
#假定 5.5 在给定X的情况下，$\epsilon|X$的条件分布为正态分布，即$\epsilon|X  \sim N(0, \sigma^2 \mathbf I_n)$。

考虑最简单的假设检验(hypothesized testing)，对单个回归系数$\beta_k$进行检验。
- 原假设$H_0$：$\beta_k = c$
- 备择假设$H_1$：$\beta_k \ne c$

> ##### 定义
> **假想值(hypothesized value)**： c，为给定常数
> **双边替代假设(two-sided alternative hypothesis)**：假设的情况即可能是$\beta_k \lt c$，也可能是$\beta_k \gt c$。
> **双边检验(two-sided test)**：假设为双边替代假设的检验。拒绝域分布在两边。 
> **沃尔德检验(Wald test)**：直观地，如果未知参数 $\beta_k$ 离 c 较远，更倾向于拒绝原假设。

那么，根据 #假定 5.5，且$\hat \beta - \beta=(X'X)^{-1}X'\epsilon = \mathbf A \mathbf \epsilon$是$\epsilon$的线性函数。所以$$(\hat \beta - \beta)|X  \sim N(0, \sigma^2 (X'X)^{-1})$$
单独只考虑其中一个分量，有：$$(\hat \beta_k - \beta_k)|X  \sim N(0, \sigma^2 (X'X)_{kk}^{-1})$$
如果原假设 $\beta_k = c$ 成立，有：$$(\hat \beta_k - c)|X  \sim N(0, \sigma^2 (X'X)_{kk}^{-1})$$
如果 $\sigma^2$ 已知，通过标准化的统计量服从标准正态分布$$z_k \equiv \frac{\hat \beta_k - c}{\sqrt{\sigma^2 (X'X)_{kk}^{-1}}} \sim N(0,1)$$
> ##### 厌恶参数
> 通常 $\sigma^2$ 是未知的，虽然我们对 $\sigma^2$ 不感兴趣，但是它却出现在表达式里面，所以被称为厌恶参数（nuisance parameter）。

合格的检验统计量（test statistic）,必须满足两个条件：
1. 能够根据样本数据计算出来
2. 它的概率分布已知
用估计量$s^2$来替代 $\sigma^2$ 就可以得到 **t 统计量(t-statistic)**：$$t \equiv \frac{估计量-假想值}{估计量的标准误} $$
t 统计量度量估计量（$\hat \beta_k$）距离假想值（c）的距离，并以估计量的标准误（$SE(\hat \beta_k)$）作为距离的度量单位，即距离为 t 个标准误。
### （2）t 检验
#定理 【t统计量的概率分布】在 #假定 5.1-5.5均满足的情况下，且原假设“$H_0$：$\beta_k = c$”也成立，t统计量服从自由度为（n-K）的t分布：$$t_k  \equiv \frac{\hat \beta_k - c}{SE(\hat \beta_k) } \sim t (n-K)$$
#### 1. t 检验的步骤
- 第一步：计算 t 统计量，记为$t_k$。
	- 若原假设成立，$|t_k|$大概率很小
	- 若备择假设成立，$|t_k|$很大
- 第二步：计算显著性水平为 $\alpha$ 的临界值 $t_{\alpha/2} (n-K)$，其中$$P\{T \gt t_{\alpha/2} (n-K)\} = P\{T \lt -t_{\alpha/2} (n-K)\} = \frac{\alpha}{2}$$
	- 通常取 $\alpha=5\%$
- 第三步：如果 $|t_k| \ge t_{\alpha/2} (n-K)$，则落入拒绝域（reject region），拒绝原假设$H_0$；反之，落入接受域，接受原假设$H_0$。
#### 2.计算p值
假设检验的逻辑是，如果一次抽样中发生了小概率事件，则拒绝原假设。小到何种程度，用p值来衡量。在t检验中，p值(p-value)：$$p-value = P(|T| >|t_k|)$$
> ##### p值
> 称原假设可被拒绝的最小显著性水平为此假设检验问题的p值。

p值的优势：
- 比临界值更有信息量
- 操作简便，直接与显著性水平比较，直观。
#### 3.计算置信区间
有时还需要做区间估计，即参数取值的范围。

> ##### 置信区间
> 假设置信度（confidence level）为$(1-\alpha)$，**置信区间**就是使该区间覆盖真实参数的概率为$(1-\alpha)$的取值范围。

t统计量的置信区间：$$[\hat \beta_k-t_{\alpha/2}SE(\hat \beta_k),\hat \beta_k+t_{\alpha/2}SE(\hat \beta_k)]$$
- 标准误越大，置信区间越宽，对参数 $\hat \beta_k$ 的估计越不精确
- 置信区间是随机区间，随样本不同而不同
#### 4.单边检验
有时也需要进行单边检验。
拒绝域只在概率分布的左侧或右侧。
#### 5.两类错误
在假设检验时，可能犯下两类错误：

> ##### 第 $I$ 类错误（Type I Error）
> 虽然原假设为真，但却根据观测数据做出了拒绝原假设的错误判断，即“弃真”。第$I$类错误的发生概率为：$$P(拒绝H_0 |H_0)=P(检验统计量落入拒绝域|H_0)=\alpha$$

> ##### 第$II$类错误（Type II Error）
> 虽然原假设为假，但却根据观测数据做出了接受原假设的错误判断，即“存伪”。第$II$类错误的发生概率为：$$P(接受H_0 |H_1)=P(检验统计量落入接受域|H_1)$$

- 第$I$类错误发生的概率很容易计算，但第$II$类错误发生的概率很难计算。
- 在进行假设检验时，一般先指定可接受的发生第$I$类错误的最大概率，即显著性水平，而不指定第$II$类错误的发生概率。

> ##### 功效(power)
> 称“1减去第$II$类错误的发生概率”为统计检验的功效：$$功效=1-P(接受H_0|H_1)=P(拒绝H_0|H_1)$$

功效为在原假设为错误的情况下，拒绝原假设的概率。
## 5.9 对线性假设的F检验



## 5.10 F统计量的似然比原理表达式


## 5.11 预测


## 5.12 多元线性回归的Stata命令及实例


## 本章小结


## 习题