# 第5章 多元线性回归

## 5.1 二元线性回归

### 案例说明

Cobb-Dougls生成函数：$$y_i=\alpha k_i^{\beta}l_i^{\gamma}e^{\epsilon_i}$$

两边同时取对数，可转换为线性模型：$$\ln y_i=\ln \alpha +\beta\ln k_i +\gamma \ln l_i + \epsilon_i$$
这就是二元线性回归模型。
### 代码实现

> [[Chapter_05.ipynb]]

## 5.2 多元线性回归模型

多元线性回归模型：$$y_i=\beta_1x_{i1}+\beta_2x_{i2}+\dots+\beta_Kx_{iK}+\epsilon_i \quad (i=1，\dots，n)$$
使用矩阵表示：$$y \equiv X\beta+\epsilon$$
其中：$$\mathbf X \equiv \begin{pmatrix}
1& x_{12}& \cdots & x_{1K}\\
1& x_{22}& \cdots & x_{2K}\\
\vdots&\vdots&\ddots&\vdots\\
1& x_{n2}& \cdots & x_{nK}\\
\end{pmatrix}_{n \times K}$$
$x_{i1}=1$，即可转化为常数项。
## 5.3 OLS估计量的推导

> 使用矩阵表示多元线性回归模型简洁明了。

目标函数：$$\min_{\hat \beta_1,\dots,\hat \beta_k} \sum_{i=1}^n e_i^2= \sum_{i=1}^n(y_i-\hat\beta_1-\hat\beta_2x_{i2}-\hat\beta_3x_{i3}-\dots-\hat\beta_Kx_{iK})^2$$
- 找到$(\hat \beta_1,\hat \beta_2,\dots,\hat \beta_K)$使残差平方和（SSR）最小。

分别求偏导，得到正规方程组：$$\mathbf X' \mathbf e=0$$
- 残差向量 $\mathbf e \equiv (e_1 \quad e_2 \dots e_n)'$ 与每个解释变量均正交。
- 将 $\mathbf e$ 表示为：$$\mathbf e = \mathbf y -\mathbf X \hat \beta$$
- 带入正规方程组，求解**OLS估计量**为：$$\hat \beta \equiv (\mathbf X’ \mathbf X)^{-1}\mathbf X'y$$
## 5.4 OLS的几何解释

### 正交性
被解释变量 $y_i$ 的拟合值（fitted value）/预测值（predicted value）为 $\hat y_i$ ，有：$$\hat y_i \equiv \hat\beta_1+\hat\beta_2x_{i2}+\hat\beta_3x_{i3}+\dots+\hat\beta_Kx_{iK} \quad (i=1,\dots,n)$$ 
用列向量表示所有个体的拟合值为$\hat y$：$$\mathbf {\hat y} \equiv \mathbf X \mathbf {\hat \beta} $$
拟合值向量与残差向量正交：$$\mathbf {\hat y}'e=(\mathbf X \mathbf {\hat \beta})'e=\mathbf{\hat \beta}'\mathbf X 'e=0$$
### 线性投影
因为$\mathbf e=\mathbf y- \mathbf X \mathbf{\hat \beta}=\mathbf y - \mathbf {\hat y}$，故：$$\mathbf y = \mathbf {\hat y}+\mathbf e$$
- 拟合值 $\hat y$ 是被解释变量 $y$ 向解释变量超平面 $X$ 的线性投影（Linear projection）
- 残差 $e$ 则是从投影处，垂直于X超平面指向 $y$ 的直线
![[5-4OLS的几何解释_投影.png]]

## 5.5 拟合优度

![[4_一元线性回归#^4adc1c]]
- 拟合优度在 $[0，1]$ 之间

通过增加解释变量数和优化新增解释变量（以及已有解释变量）的系数，都可以提高$R^2$。因此，引入校正拟合优度来对解释变量过多（模型不够简洁）进行惩罚。
### 定义
校正拟合优度 $\overline R^2$  (Adjusted $R^2$) 为：$$\overline R^2 \equiv 1 - \frac{\frac{1}{n-K}\sum_{i=1}^ne_i^2}{\frac{1}{n-1}\sum_{i=1}^n(y_i-\overline y)^2}$$
- $\sum_{i=1}^ne_i^2$的自由度（degree of freedom）n-K：n个变量受K个方程约束
- $\sum_{i=1}^n(y_i-\overline y)^2$的自由度（n-1）

> 缺点：可能是负值
## 5.6 古典线性回归模型的假定

#假定 5.1  线性假定（Linearity）
- 线性假定的本质是：回归函数是参数的线性函数
- 如果变量的边际效用不是常数，可考虑加入平方项
#假定 5.2  严格外生性（Strict exogeneity）
- 解释变量 和 被解释变量 相互独立
#假定 5.2  不存在严格多重共线性（strict multicolinearity）
- 解释变量之间是独立的

```ad-note
title:矩阵的秩（Rank）
矩阵的秩（Rank）是指矩阵中线性无关的行或列的最大数目。当一个矩阵是满秩的，意味着它的秩等于它的行数或列数中的较小者。

具体来说，对于一个 $( m \times n)$ 的矩阵 $( A )$，如果矩阵$( A )$ 的所有$ ( m ) $行（或者所有 $( n )$ 列）都是线性无关的，那么我们就说这个矩阵是满秩的。对于方阵（即行数和列数相等的矩阵），满秩意味着该矩阵是可逆的，也就是说存在一个逆矩阵$A^{-1}$ 使得 $(AA^{-1} = A^{-1}A = I)$ ，其中 \( I \) 是单位矩阵。

在统计学中，如果一个数据矩阵是满秩的，那么可以通过最小二乘法来估计回归模型的参数。

总结一下，矩阵满秩意味着矩阵中的行向量或列向量都是线性无关的，这通常与系统的可解性、系统的控制性和数据的估计能力等重要性质相关联。
```
## 5.7 OLS的小样本性质

OLS估计量 $\hat \beta$ 是样本数据的函数，也是随机变量，其分布函数为抽样分布(sampling ditribution)。

古典线性回归模型假定下，OLS估计量有如下性质：
### （1）线性性

OLS估计量 $\hat \beta$ 可视为 $y$ 的线性组合，将 $(\mathbf X’ \mathbf X)^{-1}\mathbf X'$ 视为系数矩阵，故是线性估计量。
### （2）无偏性

$\hat \beta$ 不会系统地高估或低估 $\beta$，$E(\hat \beta|X)=\beta$

证明：$$\begin{equation}\begin{split} 
\hat \beta - \beta&= (X'X)^{-1}X'y-\beta\\ 
&=(X'X)^{-1}X'(X\beta+\epsilon)-\beta\\
&=\beta-\beta+(X'X)^{-1}X'\epsilon\\
&=(X'X)^{-1}X'\epsilon
\end{split}\end{equation}$$
定义 $A \equiv (X'X)^{-1}X'$，上式两边对X求条件期望得 $E(\hat \beta|X)=\beta$
进一步还可以得到：$E(\hat \beta)=E_XE(\hat \beta|X)=E_X(\beta)=\beta$
### （3）估计量 $\hat \beta$ 的协方差矩阵



### （4）高斯-马尔可夫定理(Gauss-Markov Theorem)

### （5）对扰动项方差的无偏估计


## 5.8 对单个系数的t检验


## 5.9 对线性假设的F检验



## 5.10 F统计量的似然比原理表达式


## 5.11 预测


## 5.12 多元线性回归的Stata命令及实例


## 本章小结


## 习题